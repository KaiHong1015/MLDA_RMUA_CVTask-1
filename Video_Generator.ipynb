{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport os\nimport time\nimport gc\nimport cv2\nimport torch\nimport torchvision\nimport albumentations as A\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nfrom ensemble_boxes import *\nfrom PIL import Image\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\n%matplotlib inline\nmatplotlib.rcParams['figure.figsize'] = (30,30)\nmatplotlib.rcParams['font.size'] = 14","metadata":{"execution":{"iopub.status.busy":"2021-05-31T08:22:47.238701Z","iopub.execute_input":"2021-05-31T08:22:47.238990Z","iopub.status.idle":"2021-05-31T08:22:48.437148Z","shell.execute_reply.started":"2021-05-31T08:22:47.238959Z","shell.execute_reply":"2021-05-31T08:22:48.436299Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2021-05-31T08:22:48.438848Z","iopub.execute_input":"2021-05-31T08:22:48.439184Z","iopub.status.idle":"2021-05-31T08:22:48.457522Z","shell.execute_reply.started":"2021-05-31T08:22:48.439147Z","shell.execute_reply":"2021-05-31T08:22:48.456715Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def get_model(checkpoint_path):\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=False)\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 5)\n    \n    checkpoint = torch.load(checkpoint_path)\n    model.load_state_dict(checkpoint)\n    model = model.to(device)\n    model.eval()\n    del checkpoint\n    gc.collect()\n    \n    return model\n\n\n# models = [\n#     get_model('../input/objectdetection/fold0_resnet50_BestModel.pth'), \n#     get_model('../input/objectdetection/fold1_resnet50_BestModel.pth'),\n#     get_model('../input/objectdetection/fold2_resnet50_BestModel.pth'),\n#     get_model('../input/objectdetection/fold3_resnet50_BestModel.pth'), \n#     get_model('../input/objectdetection/fold4_resnet50_BestModel.pth'),\n#     get_model('../input/objectdetection/fold5_resnet50_BestModel.pth'),\n# ]\n\nmodel = get_model('../input/objectdetection/fold1_resnet50_BestModel.pth')","metadata":{"execution":{"iopub.status.busy":"2021-05-31T08:27:53.915604Z","iopub.execute_input":"2021-05-31T08:27:53.915942Z","iopub.status.idle":"2021-05-31T08:27:54.795470Z","shell.execute_reply.started":"2021-05-31T08:27:53.915911Z","shell.execute_reply":"2021-05-31T08:27:54.794664Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"transform = {\n    'test': A.Compose([A.Resize(height=720, width=1280, p=1.0),\n                      ToTensorV2(p=1.0)], p=1.0)\n}\n\nclass CarTestDataset(Dataset):\n    def __init__(self, image_root, depthmaps_path, transform=transform['test']):\n        self.image_root = image_root\n        self.transform = transform\n        self.image_paths = [image_root + '/' + image_root.split('/')[-1].split('_')[-3] + f'_{i}_image.jpg' for i in range(901)]\n        self.depthmaps = np.load(depthmaps_path)\n        self.isleft = True if image_root.split('/')[-1].split('_')[-2]=='left' else False\n        \n    def __getitem__(self, index):\n        image_path = self.image_paths[index]\n        image = Image.open(image_path).convert('RGB')\n        image = np.array(image).astype(np.float32) / 255.0\n        \n        tsfm = self.transform(**{'image':image})\n\n        return tsfm['image'], self.depthmaps[index], self.isleft\n    \n    def __len__(self):\n        return len(self.image_paths)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T09:09:03.327723Z","iopub.execute_input":"2021-05-31T09:09:03.328088Z","iopub.status.idle":"2021-05-31T09:09:03.336606Z","shell.execute_reply.started":"2021-05-31T09:09:03.328056Z","shell.execute_reply":"2021-05-31T09:09:03.335658Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"root_dicts = {\n    '../input/testcarimages/ally_right_images/ally_right_images': '../input/testcarimages/ally_depth_mm/ally_depth_mm.npy',\n    '../input/testcarimages/enemy_right_images/enemy_right_images': '../input/testcarimages/enemy_depth_mm/enemy_depth_mm.npy',\n    '../input/testcarimages/ally_left_images/ally_left_images': '../input/testcarimages/ally_depth_mm/ally_depth_mm.npy',\n    '../input/testcarimages/enemy_left_images/enemy_left_images': '../input/testcarimages/enemy_depth_mm/enemy_depth_mm.npy',\n}\n\nlabel2color = {\n    1: (30,144,255),\n    2: (0,255,255),\n    3: (255,69,0),\n    4: (240,128,128),\n}\n\nlabel2str = {\n    1: 'Ally Robot',\n    2: 'Ally Armor',\n    3: 'Enemy Robot',\n    4: 'Enemy Armor',\n}","metadata":{"execution":{"iopub.status.busy":"2021-05-31T09:21:36.559524Z","iopub.execute_input":"2021-05-31T09:21:36.559900Z","iopub.status.idle":"2021-05-31T09:21:36.565292Z","shell.execute_reply.started":"2021-05-31T09:21:36.559864Z","shell.execute_reply":"2021-05-31T09:21:36.564368Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"def get_distance(box, label, dm, isleft):\n    '''\n    Calculate the distance of the bounding box predicted from death map\n    \n    Input:\n    box: list or numpy -> a single predicted box\n    label: list or numpy -> a single predicted label\n    isleft: boolean -> Is it the left side camera?\n    \n    Return:\n    d: float -> The calculated distance\n    '''\n    if isleft:\n        c = 50   \n    if not isleft:\n        if label == 1 or label == 3:\n            c = round(175/366 * (box[3]-box[1]))\n        else:\n            c = round(175/45 * (box[3]-box[1]))\n        \n    xmid = round(box[0] + (box[2] - box[0])/2) + c\n    ymid = round(box[1] + (box[3] - box[1])/2)\n    \n    d = dm[ymid-10:ymid, xmid:xmid+17][dm[ymid-10:ymid, xmid:xmid+17]!=0]\n    return 0.0 if not len(d) else d.mean()\n       \ndef draw_info(np_image, boxes, labels, isleft):\n    '''\n    Draw predicted bounding box and distance on the image\n    \n    Input:\n    np_image: numpy -> an image in the numpy form\n    boxes: numpy or list -> Predicted bounding boxes of a single image\n    labels: numpy or list -> Predicted labels of a single image\n    isleft: boolean -> Is it the left side camera?\n    \n    Return:\n    np_image: numpy -> An image with info on it\n    '''\n    np_image = np_image.copy()\n    \n    for box, label in zip(boxes, labels):\n        color = label2color[label]\n        name = label2str[label]\n        distance = get_distance(box, label, dm, isleft)\n        text = name + f'({distance/1000:.3f}m)'\n        \n        cv2.rectangle(np_image, (box[0], box[1]), (box[2], box[3]), color, 3)\n        cv2.rectangle(np_image, (box[0], box[1] - 30), (box[0] + round(len(text)/19 * 230), box[1]), color, -1)\n        cv2.putText(np_image, text, (box[0], box[1] - 5), cv2.FONT_HERSHEY_PLAIN, 1.25, (0,0,0), 2)\n    return np_image\n    \ndef get_valid_prediction(boxes: np.ndarray, scores: np.ndarray, labels: np.ndarray, threshold=0.75):\n    '''\n    Get predictions which satisfied the conditions. We will choose the boxes with confidence>0.75, and set the\n    robot's box as a reference, then calculate the ymin threshold for armor's boxes, as all armor should be at the bottom half of \n    the robot's box\n    \n    Input:\n    boxes: numpy -> Predicted boxes on an single image\n    scores: numpy -> Predicted scores on an single image\n    labels: numpy -> Predicted labels on an single image\n    \n    Return:\n    valid_boxes: list\n    valid_scores: list\n    valid_labels: list\n    '''\n    boxes = boxes[scores > threshold]\n    labels = labels[scores > threshold]\n    scores = scores[scores > threshold]\n    \n    robot_boxes = boxes[np.logical_or(labels==1, labels==3)]\n    robot_labels = labels[np.logical_or(labels==1, labels==3)]\n    robot_scores = scores[np.logical_or(labels==1, labels==3)]\n    \n    armor_boxes = boxes[np.logical_or(labels==2, labels==4)]\n    armor_labels = labels[np.logical_or(labels==2, labels==4)]\n    armor_scores = scores[np.logical_or(labels==2, labels==4)]\n    \n    if len(robot_boxes) > 0:\n        ymin_ref = robot_boxes[:, 1].mean()\n        ymax_ref = robot_boxes[:, 3].mean()\n\n        armor_ymin_threshold = ymin_ref + (ymax_ref - ymin_ref) / 2\n\n        armor_labels = armor_labels[armor_boxes[:, 1] >= armor_ymin_threshold]\n        armor_scores = armor_scores[armor_boxes[:, 1] >= armor_ymin_threshold]\n        armor_boxes = armor_boxes[armor_boxes[:, 1] >= armor_ymin_threshold]\n\n    valid_boxes = []\n    valid_labels = []\n    valid_scores = []\n\n    for box, score, label in zip(robot_boxes, robot_scores, robot_labels):\n        valid_boxes.append(box)\n        valid_scores.append(score)\n        valid_labels.append(label)\n\n    for box, score, label in zip(armor_boxes, armor_scores, armor_labels):\n        valid_boxes.append(box)\n        valid_scores.append(score)\n        valid_labels.append(label)\n    \n    return valid_boxes, valid_scores, valid_labels\n\n@torch.no_grad()\ndef make_prediction(image):\n    '''\n    Make prediction on a single image\n    \n    Input:\n    image: tensor with shape (channels, height, width) #without batch_size!!!!\n    \n    Return:\n    prediction: dictionary which contain (boxes, scores, labels)\n    np_image: a numpy image of the input image\n    '''\n    np_image = np.ascontiguousarray(image.permute(1,2,0).mul(255).byte().cpu())\n    prediction = model(image.unsqueeze(0).to(device))\n    return prediction, np_image","metadata":{"execution":{"iopub.status.busy":"2021-05-31T09:21:38.507483Z","iopub.execute_input":"2021-05-31T09:21:38.507799Z","iopub.status.idle":"2021-05-31T09:21:38.527674Z","shell.execute_reply.started":"2021-05-31T09:21:38.507765Z","shell.execute_reply":"2021-05-31T09:21:38.526869Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"start = time.time()\nfor image_root, dm_path in root_dicts.items():\n    test_ds = CarTestDataset(image_root=image_root, depthmaps_path=dm_path)\n    \n    video_filename = image_root.split('/')[-1] + '.avi'\n    size = (1280, 720)\n\n    out = cv2.VideoWriter(video_filename, cv2.VideoWriter_fourcc(*'DIVX'), 15, size)\n    \n    for i, ds in enumerate(test_ds):\n        image, dm, isleft = ds\n\n        prediction, np_image = make_prediction(image)\n        \n        boxes = prediction[0]['boxes'].round().clip(min=0, max=1279).cpu().numpy().astype(np.int32)\n        labels = prediction[0]['labels'].cpu().numpy()\n        scores = prediction[0]['scores'].cpu().numpy()\n        \n        boxes, scores, labels = get_valid_prediction(boxes, scores, labels)\n        \n        np_image = draw_info(np_image, boxes, labels, isleft)\n            \n        np_image = cv2.cvtColor(np_image, cv2.COLOR_BGR2RGB)\n        out.write(np_image)\n    \n    out.release()\n\n    print(f'{video_filename} is done!!\\n')\nprint('Average FPS:', f'{(901 * 4 / (time.time()-start)):.4f}')","metadata":{"execution":{"iopub.status.busy":"2021-05-31T09:21:39.488254Z","iopub.execute_input":"2021-05-31T09:21:39.488580Z","iopub.status.idle":"2021-05-31T09:29:16.853944Z","shell.execute_reply.started":"2021-05-31T09:21:39.488542Z","shell.execute_reply":"2021-05-31T09:29:16.852360Z"},"trusted":true},"execution_count":78,"outputs":[{"name":"stdout","text":"ally_right_images.avi is done!!\n\nenemy_right_images.avi is done!!\n\nally_left_images.avi is done!!\n\nenemy_left_images.avi is done!!\n\nAverage FPS: 7.8801\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}